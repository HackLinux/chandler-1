---------------------------------
An Overview of Chandler and Spike
---------------------------------

Five Layers
===========

My current vision of Spike's architecture consists of five layers, each of
which should be independently testable.  Modules in higher layers may import
from modules in lower layers, but not the other way around.

Information flow between layers is always "commands down, events up".  This
means that a given layer may send commands to objects from lower layers,
and may send event notifications to objects in higher layers -- assuming that
an object in the higher layer requested the notification.  (Because that's
the only way an object in a lower layer can know that a given higher-layer
object exists.)

In general, it is desirable to have as little knowledge as possible flow from
one layer to another, in either direction, and it's also desirable to have
commands and events only travel between *adjacent* layers whenever possible,
minimizing interdependencies that make testing more complicated and don't
adequately protect code in one layer from changes in another.

In order from "highest to lowest", Spike's (planned) five layers are the
presentation, interaction, storage/system, domain model, and modelling layers:

Presentation Layer
    This layer is responsible for making connections between the interaction
    model and the GUI (wxPython).  For a given interaction model or domain
    model object, the presentation layer knows what widgets should represent
    them, and how to map GUI events into interaction commands, and how to
    map interaction and domain-level events into GUI operations.

    In Chandler today, this layer does not exist; its responsibilities are
    folded into the interaction model layer, which makes it impossible to test
    interaction-level operations without involving wxPython.  Ideally in Spike
    both this layer and the interaction layer should be unit-testable without
    involving wxPython to a significant degree.

Interaction Model
    This layer is responsible for managing end-user "commands", i.e.,
    operations to be performed on the domain model, and the "selections" (i.e.
    the currently visible, focused, or selected items in the user interface)
    that the commands are to be performed upon.  Effectively this includes
    the *logical* form of GUI widgets and windows and dialogs, but it only
    represents the information being manipulated, not its visual form.  (Which
    is the presentation layer's responsibility.)

    In Chandler today, this function is handled by CPIA, but there is no
    explicit notion of selections, and commands are called "CPIA Events".
    There is a hardwired connection between blocks and their visual
    representation, and if I understand it correctly, skinning is handled by
    selecting different sets of blocks, as opposed to keeping the interaction
    objects the same and having the presentation layer select different
    representations.

Storage/System
    This layer deals with persistence of objects, and other non-GUI system
    interactions.  In Chandler today, this is the repository, Twisted, and
    perhaps some other items.  Spike will largely just wrap these things to
    present a narrower API to its higher layers, so that higher layers don't
    have to know as much about these subsystems as they currently do.  Also,
    in today's Chandler, the repository is the lowest layer in terms of
    command/event flow.

    In Spike, however, it is considered "higher layer" because it will send
    commands to (and receive events from), the domain model, rather than the
    other way around.  On the one hand, this frees the domain model from having
    to be part of the repository, and on the other, it liberates the repository
    from having to support many degrees of flexibility in operation that it
    currently is required to have.  For example, the current repository
    implementation tries to be non-Python-specific and non-Chandler-specific
    in various ways that would be unnecessary if its configuration and
    requirements were driven by the application definition, rather than the
    other way around.

Domain Model
    This layer is the heart of the application's functionality.  A domain model
    consists of all of the "problem domain" classes that represent application
    content such as Contacts, Tasks, Notes, etc.  These domain objects are
    responsible for implementing their own information validation and behavior,
    including interactions with related domain objects.  But domain objects do
    not contain any user interface code, nor do they know anything about their
    storage mechanism(s), if any.

    In Chandler today, domain model objects are implemented as repository-based
    Kinds, whose data model is defined by parcel.xml files.  These objects are
    not only based on repository-provided classes, they also access the
    repository as part of their normal functioning.

    In Spike, the majority of the reasons why an object would directly use the
    repository will be eliminated, because the domain model will be represented
    by Python classes that are not dependent on the repository for obtaining
    metadata.  For most of the remaining use cases, repository access will
    occur in Spike's interaction layer instead.

Modelling Layer
    The modelling layer provides implementation support for the domain model,
    in the form of base classes, attribute descriptors, and metadata APIs for
    defining and introspecting the domain model.  The idea here is that domain
    layer code should simply declare things about its methods and attributes,
    and the modelling layer takes care of implementing those command and event
    interfaces that are needed by the storage, interaction, and presentation
    layers.  The modelling layer also provides typechecking and other
    constraint enforcement tools, including maintenance of bidirectional
    references.

    In Chandler today, much of this code exists within the repository, slaved
    to other aspects of the repository, and is not readily accessible for use
    by domain model code.  It also doesn't provide much support for a "static
    world" API, as will be discussed below in the `Two Worlds`_ section.

    The modelling layer, however, is at the very center of Spike, and will be
    the first part built.  It will be entirely independent of storage concerns,
    except insofar as it will accept commands and produce events that relate
    to storage requirements.  For example, if you define an attribute with
    the modelling layer, the modelling layer will handle event subscriptions
    requested by the storage layer, for finding out when objects of that class
    have been modified, and therefore need to be saved at next commit.  And it
    will also allow the storage layer to request notification when a not-yet-
    loaded attribute is accessed, so the storage layer can retrieve the value
    on behalf of the modelling layer.

This is my general reference model for constructing reusable application
architectures, so it's possible that this model will need further refinement
for Chandler.  For example, I haven't done much with end user-programmable
applications, so my notion of separating the presentation and interaction
layers may not be as good of an idea as I think it is in this context.  That
is, if a user is dragging and dropping blocks to create a UI, it may be
overkill in that context to distinguish between blocks' interaction and
presentation models.  On the other hand, the distinction would make automated
testing without wxPython possible, which might be more than enough reason to
justify the distinction even if there ends up being a 1:1 mapping between a
given interaction object and its corresponding presentation.


Inter-Layer Communication
-------------------------

Each of Spike's layers communicates by sending commands to the same or a lower
layer, and by sending requested event notifications to the same or a higher
layer.  (Requesting a notification is itself a command, so of course it goes
the opposite direction.)

Logically, both commands and events are function or method invocations, whose
signature is defined statically.  That is, it is known at coding time what
the inputs and outputs of these functions and methods are.  Each layer defines
what its own command and event interfaces are, and is responsible for knowing
how to call the command interfaces (and respond to the event interfaces) of the
layer(s) below it.  Of course, a layer can't just make up whatever interfaces
it likes; its interfaces are driven by the requirements of its superior layers.

Spike's event model will be based on the loosely-coupled observation framework
of the PyDispatcher package (see http://pydispatcher.sf.net/ ), which handles
weak references and garbage collection of event subscriptions when they
are no longer needed.  The PyDispatcher model also eliminates the need for
individual objects to keep track of their own subscriptions, which makes it
very easy to add event support to an object.

Spike does not actually use PyDispatcher itself, however, as there are some
areas where a slightly different mechanism or policy is needed.  For example,
Spike needs to be able to mark events as "consumed", which is something
PyDispatcher doesn't natively provide.  Also, PyDispatcher offers some
convenience features that Spike may not need and which could be detrimental for
performance, given Spike's high event volume.  So, Spike uses a simpler
implementation, which is found in ``spike.events``, and its documentation can
be found in ``events.txt`` in the ``spike`` package directory.


Two Worlds
==========

Within these four layers, there are two fundamentally different kinds of APIs
at work: the static world API, and the dynamic world API.

Within the domain model and parts of the interaction model, code is written to
static APIs.  You know at the time you write the code what kinds of objects
you're working with, or at least what their interfaces are, and you therefore
are using domain-specific methods and attributes.  For example, you might ask a
Contact for its name or ask an Event to reschedule itself.

Within the rest of the layers, however, one is writing general-purpose code
that doesn't really "understand" the application.  In this "dynamic world", the
code is metadata-driven.  A field editor in the interaction model doesn't
really "know" what a Contact or Event is, and it doesn't really care!  It has
an interface it expects to work with, that allows getting or setting values and
subscribing to change notifications (e.g. for when a field is calculated in
terms of other fields.)

This effectively means that the domain model needs to be able to live in both
the static and dynamic worlds.  It should be possible for static-world code
to access domain objects, and it should be possible for other layers' code to
deal with domain objects on a dynamic basis.  This means that a key requirement
for the domain model is that it must include metadata describing its structure,
so that other layers do not have to "understand" a specific application.

In addition, one of Chandler's goals is to allow end-users to define new
attributes and treat them in the application largely as if they were Chandler-
defined attributes.  Logically, these user-defined attributes cannot be part
of the "static world", because they cannot be known at the time Chandler's code
is written.  They must, therefore, live strictly in the dynamic world.  And
conversely, users cannot modify Chandler's "static world" attributes, since it
would by definition break Chandler's static-world code.

In Chandler's current model, the static model is underpowered, and it is
subordinate to the dynamic model.  For example, the repository controls the
metadata that describes the domain model, so the domain model can't even access
its own metadata without using the repository, which first has to be loaded
with schema information contained in separate files.  This makes static API
programming (the development of domain objects and services) harder to unit
test, because it is therefore always dependent upon the repository.

In Spike, this state of things is turned around, so that the domain model and
its modelling-layer metadata are at the conceptual center of the architecture.
While it's well-understood that Chandler's current architecture was intended to
support dynamic, user-defined attributes, this does not need to be the
architecture's focus.  It's more than sufficient to treat user-defined
attributes as data that can be associated with any content item, without
needing them to be part of the static API as well.  Non-static layers will see
such attributes as if they were any other attribute, but static layers will
only see them through a dynamic API -- which only makes sense, because by
definition, user-defined attributes can't be known ahead of time.

For example, if part of the domain model's dynamic API is a method to get the
value of a specified attribute, it can also function for both static and
user-defined attributes.  However, for statically-defined attributes, code can
also use ``someObject.someAttribute`` to get the attribute value.

Such code *cannot* reference user-defined attributes in such a way, but not
only because that code doesn't know what user-defined attributes will exist
ahead of time.  Spike's architecture must actually make a stronger guarantee:
it must *never* be possible to access a user-defined attribute (UDA) via normal
Python attribute access directly from a content item, because it would
otherwise be more difficult to evolve Chandler's schema safely.

Right now, if a user were to add an attribute to a Chandler-defined "kind",
and a future version of Chandler added an attribute to that kind with the
same name as the one the user added, Chandler would be forced to rename
the user's existing attribute in order to avoid conflict.  However, if UDA's
always exist only in a dynamic namespace with no direct mapping to Python
object attribute names, then there is never any possibility of conflict.


Information Modelling
=====================

Spike's modelling layer will use a metamodel similar to the one already used
in Chandler, but with some important simplifications, and with some terminology
revisions to better align with existing metamodel standards and terminology.

There will be two fundamentally different kinds of objects in Spike, just as
there are in Chandler today.  Chandler's repository calls them Kinds and Types,
but in Spike they will be known as Entities and Values.  An entity is a
mutable persistent object with potentially bidirectional references, and a
value is an immutable object that has unidirectional references only to other
values, and which may be persisted only as an attribute of an entity.

(This terminology is chosen to reflect the different roles these objects play
in representing real-life objects, and is common industry terminology, as used
in e.g. the "Value Object" pattern.  Entities are used to model real-world
things, and values are used to represent facts regarding those things.)

Chandler currently requires that all references be bidirectional in both the
static and dynamic APIs, but this places a challenge on extenders of Chandler,
who may need to have their new content kinds refer to items of an existing
kind.  This means they need to be able to modify the existing kind, which can
lead to naming conflicts if multiple extenders end up using the same attribute
name on the existing kind.  And of course this all assumes that they are
allowed to modify the existing kind at all!

Spike will therefore need to allow defining relationships without modifying
an existing kind in a way that's represented in that kind's static API, and
provide a mechanism for navigating from an instance of an existing kind, via
a dynamically-selected relationship.  Thus, even though the association is
bidirectional, it need not be symmetrical as to the API required for access.

So, if NewKind has an attribute ``foo`` that refers to instances of OldKind,
it does not mean that OldKind must have an attribute that refers to NewKind.
However, it does mean that at runtime it will be possible to take an instance
of OldKind, and request the collection of NewKind items that reference the
OldKind instance in their ``foo`` attribute.

Also, Chandler currently deals in three kinds of relationship cardinality:
"single", "list", and "dict".  Spike will be trying to replace "list" with
"set", although it may not be possible to do so in 100% of existing uses of
"list".  However, "set" will be preferred to "list", because manually-ordered
collections place additional burdens on the underlying storage mechanism, and
make event notifications more complex (because order changes and insert/delete
locations have to be tracked).

Also lists potentially allow an object to be referenced more than once in the
same collection, which adds further complexity. So, in Spike, it will be easier
to define a "set" relationship than a "list" one; the option or API will be
more verbose in order to discourage casual use.  The only places that lists
are needed instead of sets are in applications where the user is allowed to
manually order items in the collection, rather than being displayed in some
sort order, or where there is data imported from an external source that must
remain in a particular sequence.

The Z-order and tabbing-order of GUI widgets, and the attachments of an e-mail
message are good examples of these use cases.  However, such ordering is
unlikely to be manipulated fromdomain-model code directly; it's more likely to
be established in some external format or via the UI itself.  So, it may be
that simply including sort fields and manipulating them via the UI or during
the import process may be more than sufficient.  An imported sequence is
unlikely to need changing, so simply assigning sequence numbers suffices.  A
manually-assigned ordering is likely to be over a relatively small number of
items, such that renumbering them to change sequence could be cheap.

However, there may be use cases I haven't anticipated here, so I leave open the
possibility that Spike will support the "list" cardinality.  On the other hand,
that support may be simply by making it very easy to iterate over a set in
a particular sort order, or to renumber items to match a specified sequence.


Platform Extensibility
======================

As Chandler becomes a widely-used development platform, various extensibility
and scalability issues come into play.  For example, application startup time
is currently proportional to the number of parcels installed, because Chandler
has to scan for an ever-increasing number of ``parcel.xml`` files, and make an
ever-increasing number of ``startupParcel()`` calls.

In contrast, the Eclipse platform uses an approach that avoids activating
plugins until and unless that plugin is actually performing some function,
which allows a user to install large numbers of plugins without significantly
affecting startup time.  Of course, plugins that actually need to start with
the platform exist, but Eclipse's plugin API is designed so as to make it easy
to avoid unnecessary early startup.

Also, because Chandler involves a possibly-evolving data model, managing schema
evolution will play a big part in Chandler's extensibility story.  Spike will
experiment with some different ways of handling both of these issues, such as
by using Spike's event mechanism to issue "install", "uninstall", and "upgrade"
events to parcels, to allow them to perform any necessary repository
maintenance, and to manage inter-parcel version dependency issues.  (E.g.
prohibiting the uninstallation of a parcel that another parcel depends on.)

Spike will also attempt to delay importing a parcel's code until either one of
its operations is used, or an object of one of its types is accessed, thus
helping to amortize startup cost over a longer time period, and to move the
cost of unused parcels closer to zero.

For example, parcels containing Twisted servers should not hook Chandler
startup in order to register their servers with Twisted. Instead, a mechanism
should be provided for putting server definitions into the repository.  These
definitions would designate the parcel class to be used for actually managing
connections for that server, so the parcel would not actually be started until
a connection of the appropriate type is received, thus avoiding the need to
import the parcel at Chandler startup.

In general, this will be handled via "delegates".  That is, by having objects
that implement a required interface by importing the actual implementation and
then delegating to it.  These "delegates" will then be registered at startup
using information from the repository, rather than first starting the parcel
and registering the objects directly.


Spike's Package Layout
======================

Note: this is a work in progress; much is still TBD, and this is not an
exhaustive list of the modules and packages that will appear in Spike, just the
ones that I'm reasonably sure of at the moment.  Also, not all packages listed
will actually be created or "finished" during the initial prototyping period.

* Modelling Layer

  ``spike.events``
    Support for defining event types, and for subscribing to events

  ``spike.models``
    Base classes and support routines for "observable" objects and collections,
    so that the domain, storage, and UI-related layers can observe domain-model
    objects.

  ``spike.schema``
    Attribute descriptors and other support for defining metadata to describe
    domain model objects for purposes of constraint checking, and introspection
    by other layers.

* Domain Model

  Domain model code will go under a ``chandler`` package, with subpackages and
  modules named according to the corresponding entities in today's ``osaf.*``
  packages.  In a completed system, this would of course be the largest body of
  code, but in Spike's initial prototyping this will be relatively narrow,
  as there will be time only to implement a few illustrative content types.

* Storage/System

  ``spike.storage``
    Base classes for workspaces, an in-memory workspace implementation, and
    workspace->repository interfacing.  May become a subpackage with undo/redo
    stuff, depending on time/resource availability.  UUID stuff will also go
    here, along with any schema evolution mechanisms and tools.

  ``spike.services``
    Support for "background" tasks and services via Twisted, and inter-parcel
    service registration and lookup.

* Interaction Layer

  ``spike.ui``
    Subpackage with modules for menus, commands, and other interaction model
    concepts.

* Presentation Layer

  ``spike.ui.views``
    Subsystem for registering and retrieving presentations for commands and
    other interaction-model concepts.

  ``spike.ui.wx``
    wxPython-specific presentation implementations.



Misc. Notes
===========

This section is just in-progress requirements and design notes that will get
moved to the documentation for the corresponding modules and packages as those
subsystems are created.


Interaction Requirements
------------------------

Command visibility and enabledness, based on view visibility and contents'
selectedness.

Do we need to activate/deactivate commands in groups? (like Eclipse's command
sets)  In principle, no, but what if large groups change activation frequently?
Will probably need to balance when/where command applicability is checked so as
to avoid repetitive checking.


Presentation Requirements
-------------------------

Map commands to keyboard accelerators, menu text and menu positions,
toolbar buttons, icons, ordering.

Map drag-and-drop operations to commands.


Workspaces
----------

Add/remove item to workspace generates set add/remove events for corresponding
extents.


Schema
------

Association/association-end needs a concrete representation (to support dynamic
association access), but it should still be easy to define simple uni- or bi-
directional associations without a lot of overhead.  Ideally, it should be
possible to define a bidirectional relationship without needing to use naming
or import tricks to make the link.  Instead, we could maybe have something
like::

    class inheritance(schema.Relationship):
        """Subclass/superclass relationship"""
        subclasses   = schema.Many()
        superclasses = schema.Many()

    class Kind(schema.Entity):
         subclasses = inheritance.subclasses
         superclasses = inheritance.superclasses

thus allowing bidirectional relationships to be defined without needing to
explicitly specify the type.  (Possible downside: if multiple types allowed to
participate in a given end of the relationship, this may reduce type safety.)

Referencing a type instead of one end of a "relationship" would implicitly
create a relationship with an unnamed reverse end; in this way the dynamic API
always has a relationship object to use for looking things up.

In principle, this would also allow creating standalone bidirectional
relationships, e.g.::

    class likes(schema.Relationship):
        """People a contact likes"""
        liker = schema.Many(Contact)
        liked = schema.Many(Contact)

Then, one could iterate over liker/likes pairs, or use dynamic relationship
navigation to go from a person to their liked people or vice versa.  This
would allow parcels to create relationships between existing types without
affecting those types' static API.


Schema Evolution
~~~~~~~~~~~~~~~~

For schema evolution, every data aspect (class, relationship, role, attribute)
needs a UUID.  Code with UUIDs written out inline is ugly, but safest.  Next
safest is to define constant UUIDs, and include them near the top of the
module -- but this is still ugly since there will be many of them, forcing you
to scroll down to find the meat of the module.

Putting the UUIDs at the end of the file is less safe, because checking comes
later, or if it's done via assignment then some edits may result in the UUID
staying with the old name, when it should go to the new name.

In essence the problem is, "How can we have a visible token that stays with
the schema element without being visible, and that is independent of the name
yet doesn't have to be independently maintained?"

Hm.  What if UUID's have to appear in an element's docstring somewhere?  It's
not a perfect solution, but it puts it a little more out of the way.  The
ugliness of the UUID would encourage the coder to put more text in the
docstring to balance out the appearance.  A UUID generation tool could easily
add them to a docstring by the user including a token like "UUID:???".

The downside to this approach is that it's still quite verbose, and it forces
you to bulk up docstrings for things that are often quite self-explanatory
already.

If I were doing this by hand, I'd probably use the approach of putting
symbolic constants at the beginning of the file, or using some kind of
"fingerprint" technique wherein each schema element used only a 16-bit or
32-bit hex marker to identify the actual UUID, e.g.::

    class Item:
        displayName = schema.One(String, 0x454E)
        monitors = schema.One(object, 0x43A9)
        queries = schema.Many(object, 0xAF0D)
        issues = schema.Many(String, 0xB941)
        examples = schema.Many(String, 0x8725)
        description = schema.One(String, 0x33C9)

The actual UUIDs could then be looked up elsewhere.  This is is still ugly,
but it beats putting 128-bit UUIDs in the code.

On the other hand, maybe there's no need for UUIDs for anything but classes
and standalone relationships.  It would be nice to be able to rename or
otherwise mess around with individual attributes without affecting the actual
schema, but in actual usage, wouldn't it be the case that you need to "upgrade"
the physical schema anyway?  Maybe all you really need is a package revision
identifier, and when syncing the Python schema definition to the repository
schema definition, you can check whether the Python schema is different from
the physical one, assuming that the version info is the same.  If the version
info is the same, but the schema is different, you kick out a message to the
programmer telling them to bump the version number and either write a schema
upgrade, or else to uninstall and reinstall the parcel from that repository.

Running with that theory for a moment, there are a couple of holes.  First,
there needs to be some way to associate the parcel version with the schema
item, but that could be as simple as requiring the module to include a parcel
specifier of some kind (such as a UUID).  Second, the core schemas have lots
of bidirectional relationships, so requiring a UUID for every Relationship
would be like requiring UUIDs for half of all attributes.

So, probably the simplest approach is:

* Require one UUID to denote the parcel (possibly automatically determined
  based on package name if no containing package has one)

* Require specification of a parcel schema version (defaulting to 0)

* For each schema element needing a UUID which does not have one explicitly
  supplied, automatically generate a UUID using the parcel UUID as a namespace

* Complain loudly when a match can't be found

This will work very nicely without the user needing to care about UUIDs at all
unless they rename/reorganize things without changing the schema, in which case
the new autogenerated UUIDs won't match up with the old, and they'll be forced
to write a meaningless upgrade script.  However, in that case they can always
look up the old UUID and and put it on the moved class or whatever.  So in
this model, renaming or reorganizing causes UUIDs to accumulate on the schema
like barnacles, allowing you to tell the age of a piece of code by the number
of UUIDs in it.  :)

All of this just illustrates the basic idea that schema evolution inherently
sucks, and that if you change your application, you are doomed to write upgrade
scripts.  OTOH, that also indicates that the problem we *should* be solving is
how to write upgrade scripts, as in this context UUIDs just dodge the need to
write a certain subset of upgrade scripts.  (That is, those that involve moving
or renaming parts of a schema.)  Also, in the context of guaranteed UUIDs, you
can also be sure when something is just added, and thus eliminate another class
of upgrade script (e.g., ones where a new field's default value is adequate).
So the really interesting upgrades have to do with removal of information, and
transformation of existing information.

More interesting than the concept of schema equivalence is the concept of
schema conformance.  For example, suppose a user installs version 23 of parcel
Q, and then uninstalls it and reverts to version 15 of that package, because
they prefer it.  Ideally, the system shouldn't disallow this if version 23 only
added some new fields or renamed existing fields.

But that feature is very hard to do without real UUIDs, so that seems to put
me right back where I started.  Sigh.

Okay, so I guess my first idea of putting the UUIDs in a separate file was the
best way after all.  You have to edit it when you reorganize or rename, but
if you forget then a UUID will be missing on some item.  The only tricky bit
is to distinguish between the cases of "renamed item + new item w/old name" and
"new item".  Oh, and distinguishing between "renamed item" and "1 item added,
1 item deleted" is also tricky.  The thing we want to avoid here is somebody
just rerunning the UUID generation tool and screwing something up, when they
should be manually editing the file to change the relevant information.

But, maybe the way we can deal with that is to also look at the repository to
find orphaned UUIDs.  Then, the system can reverse-lookup these UUIDs in the
config file, so as to determine the names of possibly removed or renamed items.
Then, the error message can suggest editing the UUID file, and the generation
tool can refuse to generate new UUIDs until all orphans have been reassigned
or marked for deletion in some way.

This scheme still requires some way to designate parcel and version, and still
needs upgrade script support, but makes it easy to recognize what parts of
a schema are the "same" even if they have different names or locations.  It
also suggests that the system can force a new UUID to be used whenever a
backward-incompatible change is made, and add an entry to the configuration
file under the old UUID describing the upgrade/downgrade procedure for the
changed item.  It would also allow the system to warn you when installing
a new version of some parcel that you won't be able to reverse the procedure.


Domain Events
-------------

Division of repsonsibilities:

* Entity is role->set mapping (single, dict, and other cardinalities can be
  treated as a special case of sets)

* Sets (incl. special cases) issue post-action events after add or delete item;
  rollback issues reverse event.  (This makes the event model really simple;
  no need for pre-op validation and other event hooks.  OTOH, see below re:
  sync vs. async events and txn./undo logs; maybe we should distinguish between
  sync. validation events and async notices, since we can then push e.g. UI
  or DB errors out of the realm of sets' processing.  But that idea can wait
  until a later stage of development.)

* Requesting a currently unknown role for an entity triggers a load event for
  that role, falling back to a role-defined calculation, default, or empty set
  (Storage system subscribes to load events in order to lazily supply
  persistent data.)

* Attribute descriptors know their roles, and can cache the sets in entity
  __dict__

* Operation failures due to validation should fire validation events for the
  set; entities should link set validation failures to entity-level validation
  events

Misc. issues:

* how should unhandled exceptions in event receivers be treated?  Log and
  suppress, or crash safely?  Probably the sane thing to do is to force a
  shutdown, since the system's sanity can't be guaranteed otherwise.  There
  are probably a couple of different ways to do that, such as having an error
  handling method in the event classes, and/or having a CallbackError event
  class, so the platform can have listeners that handle the situation.

  At a more basic level, components can ensure their own integrity by wrapping
  event sends in try-except-finally blocks such that they roll back work in
  progress if an exception occurs.

* At what point do failure events become exceptions?  Or does the system just
  track "active failures" and refuse commit if any?  (In general, validation
  seems to need more thought.)

* Event aggregation and suppression (probably can be handled via simple
  delegates; we'll see when we get to UI level, and it's not clear to me that
  Chandler currently needs this anyway)  Idea: "async" events vs. synchronous
  events.  I.e., async events can be queued, aggregated, and even cancelled
  before sending.  Interestingly, this has strong similarities to an undo log;
  i.e., it's useful to aggregate and merge redundant diffs.

* Event for object loaded w/outdated schema (this probably doesn't actually
  make any sense w/latest schema evolution plan; it can probably be subsumed
  under normal "load a set/compute default" events)

* workspace loaded event (monitor object states to roll back upon error?)

* undo log via event listeners?  (Snapshot would suffice for single-item sets
  and small sets, but large sets could reasonably use diffs.  OTOH, diffs
  would always work, and therefore be simpler.  Not clear yet how this would
  work though.)

* Idea: receiver prioritization (via override getReceivers()) and uniqueness
  (e.g. create a ``Hook`` subclass of ``Event`` for events that should only
  have a single subscriber for a given sender at any one time, and perhaps
  ``Callback`` for hook events that should only be called once.

